{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from drun.metrics import send_metric, Metric\n",
    "from drun import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "init('recognize digits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99        88\n",
      "          1       0.99      0.97      0.98        91\n",
      "          2       0.99      0.99      0.99        86\n",
      "          3       0.98      0.87      0.92        91\n",
      "          4       0.99      0.96      0.97        92\n",
      "          5       0.95      0.97      0.96        91\n",
      "          6       0.99      0.99      0.99        91\n",
      "          7       0.96      0.99      0.97        89\n",
      "          8       0.94      1.00      0.97        88\n",
      "          9       0.93      0.98      0.95        92\n",
      "\n",
      "avg / total       0.97      0.97      0.97       899\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[87  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 88  1  0  0  0  0  0  1  1]\n",
      " [ 0  0 85  1  0  0  0  0  0  0]\n",
      " [ 0  0  0 79  0  3  0  4  5  0]\n",
      " [ 0  0  0  0 88  0  0  0  0  4]\n",
      " [ 0  0  0  0  0 88  1  0  0  2]\n",
      " [ 0  1  0  0  0  0 90  0  0  0]\n",
      " [ 0  0  0  0  0  1  0 88  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 88  0]\n",
      " [ 0  0  0  1  0  1  0  0  0 90]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADuCAYAAAAZZe3jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngGQIbgBIhuAGgGQIbgBIhuAGgGT+H65gYDsww8LzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f21c070db38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# The data that we are interested in is made of 8x8 images of digits, let's\n",
    "# have a look at the first 4 images, stored in the `images` attribute of the\n",
    "# dataset.  If we were working from image files, we could load them using\n",
    "# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n",
    "# images, we know which digit they represent: it is given in the 'target' of\n",
    "# the dataset.\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "\n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "expected = digits.target[n_samples // 2:]\n",
    "predicted = classifier.predict(data[n_samples // 2:])\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:4]):\n",
    "    plt.subplot(2, 4, index + 5)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Prediction: %i' % prediction)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = digits.target[n_samples // 2:]\n",
    "predicted = classifier.predict(data[n_samples // 2:])\n",
    "accuracy = metrics.accuracy_score(expected, predicted)\n",
    "\n",
    "send_metric(Metric.TEST_ACCURACY, accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(df):\n",
    "    return df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(v):\n",
    "    image_file = v['image'].convert('L')\n",
    "    image_file = image_file.resize((8, 8), Image.BOX)\n",
    "    array = np.asarray(image_file, dtype=np.uint8)\n",
    "    array = list(array.reshape(64))\n",
    "    array = np.array([(255-x) * 1.0 / 10 for x in array], dtype=np.float)\n",
    "    return {'digit': int(classifier.predict([array])[0])}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'digit': 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_file = Image.open('nine.png')\n",
    "\n",
    "data_frame = pd.DataFrame([{'image': image_file}])\n",
    "\n",
    "digit = predict(prepare(data_frame))\n",
    "digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import drun.io\n",
    "model = drun.io.export('/drun/image_recognize.model', \n",
    "                       lambda x : predict(x),\n",
    "                       prepare_func=lambda x : prepare(x),\n",
    "                       input_data_frame=data_frame,\n",
    "                       version='1.0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
