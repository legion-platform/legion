kind: ConfigMap
apiVersion: v1
metadata:
  name: "{{ .Release.Name }}-dags"
data:
  example_python_work.py: |-
    # -*- coding: utf-8 -*-
    import datetime
    import logging

    import airflow
    from airflow.operators.bash_operator import BashOperator
    from airflow.operators.python_operator import PythonOperator
    from airflow.models import DAG

    args = {
        'owner': 'airflow',
        'start_date': datetime.datetime.utcnow() - datetime.timedelta(hours=1)
    }

    dag = DAG(
        dag_id='example_python_work', default_args=args,
        schedule_interval='@once',
        dagrun_timeout=datetime.timedelta(minutes=2))

    def output_some_data():
        print('Output data')

    output_data = PythonOperator(task_id='output_some_data',
                                 python_callable=output_some_data,
                                 dag=dag)

    echo_1 = BashOperator(task_id='echo_1',
                               bash_command='echo $(date)',
                               dag=dag)
    echo_2 = BashOperator(task_id='echo_2',
                               bash_command='echo $(date)',
                               dag=dag)

    echo_1 >> output_data
    echo_2 >> output_data

    if __name__ == "__main__":
        dag.cli()

  s3-connection-test.py: |-
    """DAG for S3 connection testing."""
    from datetime import datetime
    import logging

    from airflow import DAG
    from airflow.operators.python_operator import PythonOperator

    from legion_airflow.hooks.s3_hook import S3Hook
    import os

    S3_CONN_ID = 's3_conn'
    TEST_FILE_NAME = 's3_check.csv'

    LOG = logging.getLogger('airflow.task')


    def create_file_in_s3():
        """
        Create file in S3

        :return: None
        """
        LOG.info("Create s3 hook")
        s3_hook = S3Hook(S3_CONN_ID)

        LOG.info("Create file {} in s3".format(TEST_FILE_NAME))
        file_created = {'file_created': datetime.now().isoformat()}
        s3_hook.write_json_file(obj=file_created, bucket=os.environ['S3_BUCKET_NAME'], key=TEST_FILE_NAME)
        LOG.info("Done.")


    dag_test = DAG(dag_id='s3_connection_test',
                   description='DAG creates file in S3',
                   schedule_interval='@once',
                   start_date=datetime.combine(datetime.now(), datetime.min.time()),
                   catchup=True)

    task = PythonOperator(
        task_id='create_file_in_s3',
        dag=dag_test,
        python_callable=create_file_in_s3)
