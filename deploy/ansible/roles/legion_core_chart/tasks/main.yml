---

# Create Jenkins IAM role for airflow s3 access
- name: Generate trust policy document
  template:
    src: "trust_policy.yaml.j2"
    dest: "{{ tmp_dir }}/trust_policy.{{ enclave }}.{{ cluster_name }}.yaml"
  with_items: "{{ enclaves }}"
  loop_control:
    loop_var: enclave

- name: Generate airflow s3 access policy document
  template:
    src: "airflow_s3_access_policy.yaml.j2"
    dest: "{{ tmp_dir }}/airflow_s3_access_policy.{{ enclave }}.{{ cluster_name }}.yaml"
  with_items: "{{ enclaves }}"
  loop_control:
    loop_var: enclave

- name: Create Airflow S3 access role
  iam:
    iam_type: role
    name: "{{ cluster_name }}-jenkins-role"
    trust_policy_filepath: "{{ tmp_dir }}/trust_policy.{{ enclave }}.{{ cluster_name }}.yaml"
    state: present
  with_items: "{{ enclaves }}"
  loop_control:
    loop_var: enclave

- name: Attach Airflow S3 accesse policy to the role
  iam_policy:
    iam_type: role
    iam_name: "{{ cluster_name }}-jenkins-role"
    policy_name: "{{ cluster_name }}-jenkins-airflow-s3-access-policy"
    policy_document: "{{ tmp_dir }}/airflow_s3_access_policy.{{ enclave }}.{{ cluster_name }}.yaml"
    state: present
  with_items: "{{ enclaves }}"
  loop_control:
    loop_var: enclave

# Install Legion core chart
- name: Get jenkins volume info
  ec2_vol:
    state: list
    region: "{{ aws_region }}"
    name: "jenkins_home_{{ profile }}"
    zone: "{{ cluster_zones.0.zone_name }}"
    tags:
      App: "jenkins_home_{{ cluster_name }}"
  register: ec2_jenkins_vol
  tags: jenkins_volume
  when: persistent_jenkins_volume

- name: Get legion-core chart status
  shell: helm --kube-context {{ cluster_name }} ls --all legion-core
  register: legion_core_deployment_status

- set_fact:
    airflow_deployed: "{{ legion_core_deployment_status.stdout_lines|length > 1 }}"

- name: Remove legion-core chart
  shell: helm --kube-context {{ cluster_name }} delete --purge legion-core
  when: airflow_deployed

- name: WORKAROUND delete pods in phase terminating
  shell: kubectl --context {{ cluster_name }} delete --grace-period=0 --force po $(kubectl --context {{ cluster_name }} get po -o wide | grep Terminating | awk '{ print $1 }') || true
  when: airflow_deployed

- name: Copy git deploy key to host
  copy:
    src: "{{ git_key }}"
    dest: "{{ tmp_dir }}/git.{{ cluster_name }}.deploy"
    owner: "{{ deploy_key_owner }}"
    group: "{{ deploys_key_group }}"

- name: Remove old GIT secret
  shell: kubectl --context {{ cluster_name }} delete secret legion-git-deploy  --ignore-not-found=true

- name: Create GIT secret
  shell: kubectl --context {{ cluster_name }} create secret generic legion-git-deploy --from-file=id_rsa={{ tmp_dir }}/git.{{ cluster_name }}.deploy

- name: Create legion configuration (values) file
  template:
    src: legion-core-values.yaml.j2
    dest: "{{ tmp_dir }}/legion-core-values.{{ cluster_name }}.yaml"
    mode: 0644
  vars:
    git_secret_name: legion-git-deploy
  with_items: "{{ enclaves }}"
  loop_control:
    loop_var: enclave

- name: Pre run with dumping
  shell: helm --kube-context {{ cluster_name }} install legion-core --name legion-core --debug --dry-run -f  {{ tmp_dir }}/legion-core-values.{{ cluster_name }}.yaml
  register: helm_core_install_output
  args:
    chdir: ../helms

- name: Save pre run result
  copy:
    content: "{{ helm_core_install_output.stdout }}"
    dest: "{{ tmp_dir }}/helm.core.{{ cluster_name }}.debug"

- name: Install legion core chart
  shell: helm --kube-context {{ cluster_name }} install legion-core --name legion-core --wait --timeout 600 -f  {{ tmp_dir }}/legion-core-values.{{ cluster_name }}.yaml
  args:
    chdir: ../helms

- name: Label default namespace with legion-component=core
  shell: kubectl --context {{ cluster_name }} label ns default legion-component=core --overwrite

- name: Dump current HELM status
  shell: helm --kube-context {{ cluster_name }} status legion-core
  register: helm_core_status_output

- name: Save HELM status localy
  local_action:
    module: copy
    content: "{{ helm_core_status_output.stdout }}"
    dest: "{{ tmp_dir }}/helm.core.{{ cluster_name }}.status"
